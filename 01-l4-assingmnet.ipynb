{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN027uam0R580gwR4EOng7x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik19-cloud/GenAI-Training/blob/main/01-l4-assingmnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "mRByP3475bs5",
        "outputId": "0ae846f0-69ff-4f24-8174-0b208c75b196"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1372137204.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# rag_pdf_summarizer.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# rag_pdf_summarizer.py\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- CONFIG -----\n",
        "PDF_PATH = \"l3-assigments.pdf\"      # change to your PDF path\n",
        "CHROMA_DB_DIR = \"chroma_pdf_db\"\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # or any other open-source LLM\n",
        "# -------------------\n"
      ],
      "metadata": {
        "id": "L2bEYXyQWVgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_pdf(pdf_path: str):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "    return split_docs\n"
      ],
      "metadata": {
        "id": "Nmthk23OWhtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_or_load_vectorstore(docs, persist_directory: str):\n",
        "    # Embeddings using SentenceTransformers\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME\n",
        "    )\n",
        "\n",
        "    # If you want to always rebuild, delete the folder beforehand.\n",
        "    if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
        "        vectorstore = Chroma(\n",
        "            persist_directory=persist_directory,\n",
        "            embedding_function=embeddings\n",
        "        )\n",
        "    else:\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=docs,\n",
        "            embedding=embeddings,\n",
        "            persist_directory=persist_directory,\n",
        "        )\n",
        "        vectorstore.persist()\n",
        "\n",
        "    return vectorstore\n"
      ],
      "metadata": {
        "id": "S1lqxu8uW3cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_llm(model_name: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",      # \"cuda\" / \"cpu\" / \"auto\"\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "\n",
        "    generate_text = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "    return llm\n"
      ],
      "metadata": {
        "id": "PkJj2y27W-e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rag_chain(vectorstore, llm):\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 5}\n",
        "    )\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",  # simplest type: stuff all relevant docs into prompt\n",
        "        return_source_documents=True,\n",
        "    )\n",
        "    return qa_chain\n"
      ],
      "metadata": {
        "id": "mYOnpMUVXA10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_document(qa_chain):\n",
        "    query = (\n",
        "        \"Provide a detailed, well-structured summary of the entire document. \"\n",
        "        \"Highlight key sections, main arguments, important data points, and conclusions. \"\n",
        "        \"Keep it concise but comprehensive.\"\n",
        "    )\n",
        "\n",
        "    result = qa_chain({\"query\": query})\n",
        "    summary = result[\"result\"]\n",
        "    sources = result[\"source_documents\"]\n",
        "\n",
        "    print(\"\\n=== SUMMARY ===\\n\")\n",
        "    print(summary)\n",
        "\n",
        "    print(\"\\n=== TOP RETRIEVED CHUNKS (for transparency) ===\\n\")\n",
        "    for i, doc in enumerate(sources, start=1):\n",
        "        print(f\"\\n--- Chunk {i} (page {doc.metadata.get('page', 'N/A')}) ---\\n\")\n",
        "        print(doc.page_content[:500], \"...\")\n"
      ],
      "metadata": {
        "id": "YZQYQCELXEJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # 1. Load and split the PDF\n",
        "    print(\"Loading and splitting PDF...\")\n",
        "    docs = load_and_split_pdf(PDF_PATH)\n",
        "\n",
        "    # 2. Build or load vectorstore\n",
        "    print(\"Creating/loading Chroma vectorstore...\")\n",
        "    vectorstore = create_or_load_vectorstore(docs, CHROMA_DB_DIR)\n",
        "\n",
        "    # 3. Load open-source LLM\n",
        "    print(\"Loading LLM (this may take some time the first run)...\")\n",
        "    llm = load_llm(LLM_MODEL_NAME)\n",
        "\n",
        "    # 4. Build RAG chain\n",
        "    print(\"Building RAG chain...\")\n",
        "    qa_chain = build_rag_chain(vectorstore, llm)\n",
        "\n",
        "    # 5. Summarize document via RAG\n",
        "    print(\"Generating summary...\")\n",
        "    summarize_document(qa_chain)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "V2GrfB_2XF-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}