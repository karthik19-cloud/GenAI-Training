{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmIUwfVY/Brv6d/8rqf//+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik19-cloud/GenAI-Training/blob/main/GenAI-L4/01-pdf-summarization/rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S-8xoDrJ5DFK"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community chromadb sentence-transformers transformers pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-text-splitters"
      ],
      "metadata": {
        "id": "wDx34FNc63Ez"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"langchain==0.2.16\" \"langchain-community==0.2.16\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K60t3D_y80vf",
        "outputId": "157da831-bed1-4e81-8703-ee0c2006ffc3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-classic 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.2.43 which is incompatible.\n",
            "langchain-classic 1.0.0 requires langchain-text-splitters<2.0.0,>=1.0.0, but you have langchain-text-splitters 0.2.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.2.43 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Fixed path for Google Colab sample file\n",
        "pdf_path = \"/content/sample_data/l3-assigments.pdf\"\n",
        "\n",
        "print(\"Using PDF:\", pdf_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_FVocDQ5Ujv",
        "outputId": "3bc71347-7f1b-490e-82f3-7f046a3a5ac9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PDF: /content/sample_data/l3-assigments.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "#from langchain.chains import RetrievalQA\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# 1) Load and split the PDF into chunks\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=40,\n",
        "    length_function=len\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Loaded {len(documents)} pages, split into {len(docs)} chunks.\")\n",
        "\n",
        "# 2) Create embeddings with a Sentence-Transformers model (open source)\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# 3) Create / load Chroma vector store\n",
        "persist_directory = \"chroma_pdf_index\"\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "vectorstore.persist()\n",
        "print(\"Vector store created and persisted at:\", persist_directory)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# 4) Load an open-source text generation model for summarization via HuggingFacePipeline\n",
        "#    You can choose another seq2seq model if you like.\n",
        "llm_model_name = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "summarization_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=summarization_pipeline)\n",
        "\n",
        "def rag_answer(query: str):\n",
        "    # 1) Retrieve relevant chunks from Chroma\n",
        "    docs = retriever.invoke(query)  # <- changed here\n",
        "\n",
        "    # 2) Build context text\n",
        "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "    # 3) Build a prompt for the LLM\n",
        "    prompt = (\n",
        "        \"You are a helpful assistant. Use the context below to answer the question.\\n\\n\"\n",
        "        \"Context:\\n\"\n",
        "        f\"{context}\\n\\n\"\n",
        "        \"Question:\\n\"\n",
        "        f\"{query}\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "    # 4) Call the Hugging Face text2text model\n",
        "    out = summarization_pipeline(prompt, max_length=512, truncation=True)[0][\"generated_text\"]\n",
        "    return out, docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 5) Build a RetrievalQA chain (RAG)\n",
        "# qa_chain = RetrievalQA.from_chain_type(\n",
        "#     llm=llm,\n",
        "#     retriever=retriever,\n",
        "#     chain_type=\"stuff\",  # Stuff retrieved chunks into the prompt\n",
        "#     return_source_documents=True\n",
        "# )\n",
        "\n",
        "print(\"RAG pipeline ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3TnYXXu5VWP",
        "outputId": "970f4b65-a439-474c-e105-963a84c03f00"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2 pages, split into 6 chunks.\n",
            "Vector store created and persisted at: chroma_pdf_index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG pipeline ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def summarize_document_with_rag(chain, summary_prompt: str = None):\n",
        "#     \"\"\"\n",
        "#     Uses the RAG pipeline to summarize the document.\n",
        "#     `summary_prompt` can be customized; if None, a default is used.\n",
        "#     \"\"\"\n",
        "#     if summary_prompt is None:\n",
        "#         summary_prompt = (\n",
        "#             \"Provide a comprehensive but concise summary of this document. \"\n",
        "#             \"Focus on the main topics, key points, and any important conclusions. \"\n",
        "#             \"Write the summary in clear, well-structured paragraphs.\"\n",
        "#         )\n",
        "\n",
        "#     result = chain({\"query\": summary_prompt})\n",
        "#     answer = result[\"result\"]\n",
        "#     return answer, result[\"source_documents\"]\n",
        "\n",
        "# summary, sources = summarize_document_with_rag()\n",
        "def summarize_document_with_rag():\n",
        "    query = (\n",
        "        \"Provide a comprehensive but concise summary of this document. \"\n",
        "        \"Focus on the main topics, key points, and important conclusions. \"\n",
        "        \"Write the summary in clear, well-structured paragraphs.\"\n",
        "    )\n",
        "    return rag_answer(query)\n",
        "\n",
        "# print(\"===== DOCUMENT SUMMARY (RAG) =====\\n\")\n",
        "# print(summary)\n",
        "summary, sources = summarize_document_with_rag()\n",
        "\n",
        "print(\"===== DOCUMENT SUMMARY (RAG) =====\\n\")\n",
        "print(summary)\n",
        "\n",
        "\n",
        "\n",
        "# print(\"===== DOCUMENT SUMMARY (RAG) =====\\n\")\n",
        "# print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaibLyhy5W_m",
        "outputId": "5f24a960-4a78-4300-8a9a-aaaa41c18dc5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== DOCUMENT SUMMARY (RAG) =====\n",
            "\n",
            "A comprehensive list of GenAI bronze assignments.\n"
          ]
        }
      ]
    }
  ]
}